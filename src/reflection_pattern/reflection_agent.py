import os
from dotenv import load_dotenv  
from src.utils.completions import generate_response,struct_the_prompt,FixedFirstChatHistory,update_chat_history
from groq import Groq

# lets write Logic -- so we need a chat history, we have to append generate output from llm and than
# and send that output to critic for suggesting some changes we can repeat this process for 5-6 times
# or we can stop early we get the desired output


load_dotenv()  




BASE_GENERATION_SYSTEM_PROMPT = """
Your task is to Generate the best content possible for the user's request.
If the user provides critique, respond with a revised version of your previous attempt.
You must always output the revised content.
"""

BASE_REFLECTION_SYSTEM_PROMPT = """
You are tasked with generating critique and recommendations to the user's generated content.
If the user content has something wrong or something to be improved, output a list of recommendations
and critiques. If the user content is ok and there's nothing to change, output this: <OK>
"""


class ReflectAgent:
    """
    ReflectAgent is a conversational agent designed to interact using a generation 
    and reflection loop. It generates responses from the system LLM and evaluates 
    them using the reflection LLM to refine the output.

    Attributes:
        model (str): The name of the LLM model used for generation and reflection.
        client (Groq): The client instance to communicate with the LLM.
    """

    def __init__(self, model: str = "llama-3.3-70b-versatile"):
        """
        Initializes the ReflectAgent with the specified model.

        Args:
            model (str): The model name for LLM interactions. Default is "llama-3.3-70b-versatile".
        """
        self.client = Groq()
        self.model = model

    def _request_completion(self, history: list) -> str:
        """
        Sends a request to the LLM to generate a response based on the provided history.

        Args:
            history (list): A list of messages representing the conversation history.

        Returns:
            str: The LLM-generated response.
        """
        return generate_response(self.client, history, self.model)

    def system_llm(self, system_history: list) -> str:
        """
        Generates a response from the system LLM based on the provided system history.

        Args:
            system_history (list): A list of messages representing the system's conversation history.

        Returns:
            str: The system LLM's response.
        """
        response = self._request_completion(system_history)
        return response

    def reflect_llm(self, reflect_history: list) -> str:
        """
        Generates a response from the reflection LLM based on the provided reflection history.

        Args:
            reflect_history (list): A list of messages representing the reflection conversation history.

        Returns:
            str: The reflection LLM's response.
        """
        response = self._request_completion(reflect_history)
        return response

    def run(
        self,
        user_msg: str,
        generation_system_prompt: str = "",
        reflection_system_prompt: str = "",
        n_steps: int = 10
    ) -> str:
        """
        Executes the conversational loop between the system and reflection LLMs.

        Args:
            user_msg (str): The user's input message to start the conversation.
            generation_system_prompt (str, optional): Initial prompt for the system LLM. Defaults to "".
            reflection_system_prompt (str, optional): Initial prompt for the reflection LLM. Defaults to "".
            n_steps (int, optional): Maximum number of generation-reflection cycles. Defaults to 10.

        Returns:
            str: The final response generated by the system LLM.
        """
        
        # Append base prompts to avoid duplication
        generation_system_prompt += BASE_GENERATION_SYSTEM_PROMPT
        reflection_system_prompt += BASE_REFLECTION_SYSTEM_PROMPT

        # Initialize conversation histories
        generation_history = FixedFirstChatHistory(max_length=5)
        reflection_history = FixedFirstChatHistory(max_length=5)

        # Create initial prompts
        sys_prompt = struct_the_prompt(role="system", message=generation_system_prompt)
        user_prompt = struct_the_prompt(role="user", message=user_msg)
        ref_prompt = struct_the_prompt(role="system", message=reflection_system_prompt)

        # Update chat histories
        update_chat_history(generation_history, sys_prompt)
        update_chat_history(generation_history, user_prompt)
        update_chat_history(reflection_history, ref_prompt)

        system_llm_resp = ""
        
        # Conversational loop
        for steps in range(n_steps):
            # Generate system response
            system_llm_resp = self.system_llm(generation_history)
            sys_response_prompt = struct_the_prompt(role="user", message=system_llm_resp)
            update_chat_history(reflection_history, sys_response_prompt)

            # Generate reflection response
            critic_resp = self.reflect_llm(reflection_history)

            # Break loop if reflection response indicates completion
            if "<OK>" in critic_resp:
                break

            # Update histories with reflection response
            critic_resp_prompt = struct_the_prompt(role="user", message=critic_resp)
            update_chat_history(generation_history, critic_resp_prompt)

        return system_llm_resp


